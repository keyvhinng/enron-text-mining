{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42638, 10)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./mails_clean.csv')\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp\n",
    "#data = data.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42454, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>Message-ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>From</th>\n",
       "      <th>To</th>\n",
       "      <th>Subject</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allen-p/_sent_mail/1.</td>\n",
       "      <td>&lt;18782981.1075855378110.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>2001-05-14 23:39:00</td>\n",
       "      <td>{'phillip.allen@enron.com'}</td>\n",
       "      <td>{'tim.belden@enron.com'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Here is our forecast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>allen-p/_sent_mail/10.</td>\n",
       "      <td>&lt;15464986.1075855378456.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>2001-05-04 20:51:00</td>\n",
       "      <td>{'phillip.allen@enron.com'}</td>\n",
       "      <td>{'john.lavorato@enron.com'}</td>\n",
       "      <td>Re:</td>\n",
       "      <td>Traveling to have a business meeting takes the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allen-p/_sent_mail/100.</td>\n",
       "      <td>&lt;24216240.1075855687451.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>2000-10-18 10:00:00</td>\n",
       "      <td>{'phillip.allen@enron.com'}</td>\n",
       "      <td>{'leah.arsdall@enron.com'}</td>\n",
       "      <td>Re: test</td>\n",
       "      <td>test successful.  way to go!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>allen-p/_sent_mail/1000.</td>\n",
       "      <td>&lt;13505866.1075863688222.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>2000-10-23 13:13:00</td>\n",
       "      <td>{'phillip.allen@enron.com'}</td>\n",
       "      <td>{'randall.gay@enron.com'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Randy,   Can you send me a schedule of the sal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>allen-p/_sent_mail/1001.</td>\n",
       "      <td>&lt;30922949.1075863688243.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>2000-08-31 12:07:00</td>\n",
       "      <td>{'phillip.allen@enron.com'}</td>\n",
       "      <td>{'greg.piper@enron.com'}</td>\n",
       "      <td>Re: Hello</td>\n",
       "      <td>Let's shoot for Tuesday at 11:45.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       file                                     Message-ID  \\\n",
       "0     allen-p/_sent_mail/1.  <18782981.1075855378110.JavaMail.evans@thyme>   \n",
       "1    allen-p/_sent_mail/10.  <15464986.1075855378456.JavaMail.evans@thyme>   \n",
       "2   allen-p/_sent_mail/100.  <24216240.1075855687451.JavaMail.evans@thyme>   \n",
       "3  allen-p/_sent_mail/1000.  <13505866.1075863688222.JavaMail.evans@thyme>   \n",
       "4  allen-p/_sent_mail/1001.  <30922949.1075863688243.JavaMail.evans@thyme>   \n",
       "\n",
       "                  Date                         From  \\\n",
       "0  2001-05-14 23:39:00  {'phillip.allen@enron.com'}   \n",
       "1  2001-05-04 20:51:00  {'phillip.allen@enron.com'}   \n",
       "2  2000-10-18 10:00:00  {'phillip.allen@enron.com'}   \n",
       "3  2000-10-23 13:13:00  {'phillip.allen@enron.com'}   \n",
       "4  2000-08-31 12:07:00  {'phillip.allen@enron.com'}   \n",
       "\n",
       "                            To    Subject  \\\n",
       "0     {'tim.belden@enron.com'}        NaN   \n",
       "1  {'john.lavorato@enron.com'}        Re:   \n",
       "2   {'leah.arsdall@enron.com'}   Re: test   \n",
       "3    {'randall.gay@enron.com'}        NaN   \n",
       "4     {'greg.piper@enron.com'}  Re: Hello   \n",
       "\n",
       "                                                body  \n",
       "0                               Here is our forecast  \n",
       "1  Traveling to have a business meeting takes the...  \n",
       "2                     test successful.  way to go!!!  \n",
       "3  Randy,   Can you send me a schedule of the sal...  \n",
       "4                  Let's shoot for Tuesday at 11:45.  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data.drop(columns=['Unnamed: 0','X-cc','X-bcc'])\n",
    "data=data.dropna(axis=0,subset=['body'],how='any')\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_from_mails(message):\n",
    "  mail_addresses = re.findall(r'\\(?[A-Za-z0-9_\\-\\.]+@[A-Za-zb0-9_\\-\\.]+\\.com\\)?',message)\n",
    "  if mail_addresses != []:\n",
    "    for mail_address in mail_addresses:\n",
    "      message=message.replace(mail_address,'').strip()\n",
    "  return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar emails del body\n",
    "data['body_clean']=data['body'].map(clean_from_mails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar palabras con digitos y signos de puntuacion\n",
    "data['body_clean']=data['body_clean'].map(lambda x: ' '.join(re.sub(r'[^a-zA-Z]',' ',x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasar todo a minusculas\n",
    "data['body_clean']=data['body_clean'].map(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/kref/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords final (208):\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'also', 'please', 'would']\n"
     ]
    }
   ],
   "source": [
    "my_stopwords = stopwords.words('english')\n",
    "# Excluir palabras de una sola letra\n",
    "for i in range(ord('a'),ord('z')+1):\n",
    "  my_stopwords.append(chr(i))\n",
    "# Añadir would\n",
    "my_stopwords = my_stopwords + ['also','please','would']\n",
    "print(f'Stopwords final ({len(my_stopwords)}):')\n",
    "print(my_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_msg_stopwords(message_body):\n",
    "    words = message_body.split()\n",
    "    words = [w for w in words if w not in set(my_stopwords)]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar stopwords\n",
    "data['body_clean']=data.body_clean.apply(clean_msg_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42454, 8)\n"
     ]
    }
   ],
   "source": [
    "data=data.dropna(axis=0,subset=['body_clean'],how='any')\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/kref/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector = []\n",
    "for doc in data['body_clean']:\n",
    "  word_vector.append(doc.split())\n",
    "my_vocabulary = [item for sublist in word_vector for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['let', 'shoot', 'tuesday']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vector[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El vocabulario consta de 4047756 elementos y 44475 palabras diferentes.\n"
     ]
    }
   ],
   "source": [
    "print(f'El vocabulario consta de {len(my_vocabulary)} elementos y {len(set(my_vocabulary))} palabras diferentes.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lematizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/kref/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_body(message_body):\n",
    "  return ' '.join(lemma.lemmatize(word) for word in message_body.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_vector = []\n",
    "for doc in data['body_clean']:\n",
    "  lemma_vector.append(lemmatize_body(doc).split())\n",
    "my_lema_voc = [item for sublist in lemma_vector for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El vocabulario de lemas consta de 4047756 elementos y 40903 palabras diferentes.\n"
     ]
    }
   ],
   "source": [
    "print(f'El vocabulario de lemas consta de {len(my_lema_voc)} elementos y {len(set(my_lema_voc))} palabras diferentes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['body_lemma'] = data['body_clean'].map(lemmatize_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'let shoot tuesday'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['body_lemma'][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector = []\n",
    "for doc in data['body_lemma']:\n",
    "  word_vector.append(doc.split())\n",
    "my_vocabulary = [item for sublist in word_vector for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['let', 'shoot', 'tuesday']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vector[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'let shoot tuesday'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['body_lemma'][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similaridad de Documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tuesday'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[78]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id[\"let\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id[\"shoot\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id[\"tuesday\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in dictionary: 40903\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of words in dictionary:\",len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in word_vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(76, 1), (77, 1), (78, 1)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfModel(num_docs=42454, num_nnz=2761697)\n"
     ]
    }
   ],
   "source": [
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "print(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(76, 0.22139430207687802), (77, 0.8676738950948895), (78, 0.4451141143333412)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf[corpus[4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisis de Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.interfaces.TransformedCorpus at 0x7f2e7d288c88>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "n_clusters = 6\n",
    "clf = KMeans(n_clusters=n_clusters, max_iter=100, init='k-means++', n_init=1)\n",
    "labels = clf.fit_predict(tf_idf[corpus])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento Similaridad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kref/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:718: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity index with 42454 documents in 1 shards (stored under ./)\n",
      "<class 'gensim.similarities.docsim.Similarity'>\n"
     ]
    }
   ],
   "source": [
    "sims = gensim.similarities.Similarity('./',tf_idf[corpus],\n",
    "                                      num_features=len(dictionary))\n",
    "print(sims)\n",
    "print(type(sims))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_query_tf_idf(query):\n",
    "    query_doc = [w.lower() for w in word_tokenize(query)]\n",
    "    query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "    return tf_idf[query_doc_bow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_most_similar(mail_id, n):\n",
    "  query = data['body_lemma'][mail_id]\n",
    "  print('------------------------ [ Query ] ------------------------')\n",
    "  print(data['body'][mail_id])\n",
    "  ranking = sims[make_query_tf_idf(query)]\n",
    "  sim_w_index = list(enumerate(ranking))\n",
    "  x = [t[0] for t in list(reversed(sorted(sim_w_index,key=lambda x: x[1])))]\n",
    "  print()\n",
    "  print('---------- [ Most similar emails (index, score)] ----------')\n",
    "  print(list(reversed(sorted(sim_w_index,key=lambda x: x[1])))[:n])\n",
    "  print()\n",
    "  print('------------------------ [ emails] ------------------------')\n",
    "  for ind in x[:n]:\n",
    "    print('From: {}'.format(data['From'][ind]))\n",
    "    print('To: {}'.format(data['To'][ind]))\n",
    "    print(data['body'][ind])\n",
    "    print('---------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lucy want accurate rent roll soon possible faxed copy file fill computer write correct amount input'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['body_lemma'][45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ [ Query ] ------------------------\n",
      "Lucy,  I want to have an accurate rent roll as soon as possible. I faxed you a copy  of this file.  You can fill in on the computer or just write in the correct  amounts and I will input.\n",
      "\n",
      "---------- [ Most similar emails (index, score)] ----------\n",
      "[(2091, 1.0), (1187, 1.0), (616, 1.0), (45, 1.0), (1593, 0.39354533)]\n",
      "\n",
      "------------------------ [ emails] ------------------------\n",
      "From: {'phillip.allen@enron.com'}\n",
      "To: {'jonathan.mckay@enron.com'}\n",
      "John,  Here is our North of Stanfield forecast for Jan.   Supply    Jan '01   Dec '00   Jan '00   Sumas   900   910   815  Jackson Pr.  125    33   223  Roosevelt  300   298   333    Total Supply  1325   1241   1371  Demand  North of Chehalis 675   665   665  South of Chehalis 650   575   706   Total Demand  1325   1240   1371  Roosevelt capacity is 495.  Let me know how your forecast differs.   Phillip\n",
      "---------------------------------------------------------\n",
      "From: {'phillip.allen@enron.com'}\n",
      "To: {'tara.sweitzer@enron.com'}\n",
      "Tara,  Please make the following changes:   FT-West -change master user from Phillip Allen to Keith Holst   IM-West-Change master user from Bob Shiring to Phillip Allen   Mock both existing profiles.    Please make these changes on 1/17/00 at noon.  Thank you   Phillip\n",
      "---------------------------------------------------------\n",
      "From: {'phillip.allen@enron.com'}\n",
      "To: {'ina.rangel@enron.com'}\n",
      "I have scheduled and entered on each of your calendars a meeting for the  above referenced topic.  It will take place on Thursday, 9/28 from 3:00 -  4:00 in Room EB2537.\n",
      "---------------------------------------------------------\n",
      "From: {'phillip.allen@enron.com'}\n",
      "To: {'stagecoachmama@hotmail.com'}\n",
      "Lucy,  I want to have an accurate rent roll as soon as possible. I faxed you a copy  of this file.  You can fill in on the computer or just write in the correct  amounts and I will input.\n",
      "---------------------------------------------------------\n",
      "From: {'phillip.allen@enron.com'}\n",
      "To: {'julie.pechersky@enron.com'}\n",
      "I still use this service\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "get_n_most_similar(45,5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
